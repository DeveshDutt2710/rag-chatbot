# ğŸ“š Retrieval-Augmented Generation (RAG) Chatbot â€“ Django + OpenAI

This project is a **RAG-based customer support chatbot** built with:

- ğŸ§  Local embedding (`sentence-transformers`)
- ğŸ“¦ Vector storage (`ChromaDB`)
- ğŸ§¾ Contextual answer generation (via `OpenAI GPT-3.5/GPT-4`)
- ğŸ§± Django backend with API + web UI

---

## âš™ï¸ Setup Instructions

### 1. ğŸš€ Clone the Repository

```bash
git clone https://github.com/your-org/rag-chatbot-django.git
cd rag-chatbot-django
```

### 2. ğŸ Create a Virtual Environment

```bash
python3 -m venv venv
source venv/bin/activate
```

### 3. ğŸ“¦ Install Dependencies

```bash
pip install -r requirements.txt
```

Ensure these are installed:
```
openai
chromadb
sentence-transformers
python-dotenv
django
```

### 4. ğŸ“ Create `.env` File

Create a `.env` file in the root folder:

```env
OPENAI_API_KEY=sk-XXXXXXXXXXXXXXXXXXXXXXXXXXXX
OPENAI_MODEL=gpt-3.5-turbo
CHROMA_COLLECTION=rag_collection
CHUNK_PATH=chat/embeddings/chunks.json
```

### 5. ğŸ§© Prepare Source Data

Make sure you've already extracted and chunked content (PDFs/DOCX) and saved it as:

```
chat/embeddings/chunks.json
```

> If not yet done, run:
```bash
python scripts/parse_pdfs.py
python scripts/parse_docx.py
python scripts/build_index.py
```

### 6. ğŸ’¬ Run the Django App

```bash
python manage.py migrate
python manage.py runserver
```

Then open [http://127.0.0.1:8000/chat/](http://127.0.0.1:8000/chat/)

---

## ğŸ“¦ Project Structure

```
â”œâ”€â”€ chat/
â”‚   â”œâ”€â”€ templates/chat.html   # Frontend UI
â”‚   â”œâ”€â”€ views.py                   # API + page view
â”‚   â”œâ”€â”€ rag_pipeline.py           # Core RAG logic
â”‚   â””â”€â”€ urls.py
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ parse_docx.py
â”‚   â”œâ”€â”€ parse_pdfs.py
â”‚   â””â”€â”€ build_index.py            # Preprocess + embed
â”œâ”€â”€ ragchatbot/                      # Django settings
â”œâ”€â”€ .env
â”œâ”€â”€ requirements.txt
â””â”€â”€ manage.py
```

---

## âœ… Features

- Context-based answering from support documents only
- Answers â€œI donâ€™t knowâ€ if not found in source
- Modular: can swap OpenAI with Hugging Face, local LLMs
- Built-in script to preprocess docs and build vector index

---

## ğŸ” Notes

- Ensure your OpenAI account has active billing / quota
- This project **will incur OpenAI API costs** per request
- You can switch to Hugging Face or local models if needed

---

## ğŸ§  Optional: Use Hugging Face or Grok

We also support:
- `huggingface_hub.InferenceClient`
- `openrouter.ai` (Grok or OpenChat)

Ask if you want that version instead.
